# MoodScout Project Context

## Overview

MoodScout is a real-time acoustic behavioral analysis tool designed for call centers. It listens to a user's audio stream (microphone), analyzes the emotional state using the Google Gemini Live API, and provides real-time, tactical suggestions to the operator without the AI speaking back.

## Tech Stack

- **Frontend**: React 19, TypeScript
- **Styling**: Tailwind CSS
- **AI/Backend**: Google Gemini Live API (`@google/genai`) via WebSockets
- **Visualization**: Recharts
- **Audio**: Native Web Audio API (`AudioContext`, `ScriptProcessorNode`)

## Key Architecture Patterns

### 1. The "Silent Observer" Pattern

Unlike typical voice assistants, MoodScout is designed **not to speak**.

- We connect to Gemini Live with `responseModalities: ['AUDIO']`.
- We provide a `speechConfig` to satisfy API requirements but deliberately **ignore** the audio binary data returned in `onmessage`.
- Communication from Gemini to the App happens exclusively via **Function Calling** (`update_dashboard`).

### 2. Audio Pipeline

1. **Capture**: `navigator.mediaDevices.getUserMedia` captures raw audio.
2. **Processing**: `ScriptProcessorNode` accesses the raw PCM data buffer.
3. **Downsampling**: Browser audio (often 44.1kHz or 48kHz) is downsampled to **16kHz** using `downsampleTo16k` in `utils/audioUtils.ts`.
4. **Encoding**: Converted to 16-bit PCM (`floatTo16BitPCM`), Base64 encoded, and sent to Gemini via `session.sendRealtimeInput`.

### 3. State Management

- **`useGeminiLive` Hook**: This is the brain of the application.
  - Manages the WebSocket connection state (`connected`, `connecting`, `disconnected`).
  - Holds `currentAnalysis` (latest frame) and `history` (time-series data).
  - Handles the difference between `continuous` and `single_shot` modes.
  - **Ref Pattern**: Extensive use of `useRef` (e.g., `modeRef`, `sessionPromiseRef`) to access mutable state inside async callbacks (like `onmessage` or `onaudioprocess`) without stale closure issues.

## Project Structure

### Core Logic

- **`hooks/useGeminiLive.ts`**: Handles audio streams, WebSocket connection, tool execution, and cleanup.
- **`utils/audioUtils.ts`**: Low-level audio buffer manipulation.
- **`constants.ts`**: Contains the `SYSTEM_INSTRUCTION` which defines the AI's persona and JSON schemas.

### Components

- **`App.tsx`**: Main layout and mode toggling.
- **`EmotionBadge.tsx`**: Visual representation of the current emotion with confidence bars.
- **`SuggestionsPanel.tsx`**: Textual advice generated by the AI.
- **`EmotionChart.tsx`**: A `recharts` line graph showing confidence volatility over time.

### Data Types (`types.ts`)

- **`EmotionType`**: Enum of detected emotions (Anger, Stress, Confusion, etc.).
- **`AnalysisData`**: The payload structure expected from the `update_dashboard` tool call.
- **`AnalysisMode`**: `'continuous'` | `'single_shot'`.

## Critical Implementation Details

- **Tool Definition**: The `update_dashboard` function declaration in `useGeminiLive.ts` is the strict schema the AI must adhere to.
- **Safety**: The `disconnect` function ensures all audio tracks and contexts are closed to stop the microphone light on the user's device.
- **Single Shot Logic**: In `onmessage`, if `modeRef.current === 'single_shot'`, the app automatically calls `disconnect()` after processing the first tool call.

## Development Notes

- When modifying audio logic, ensure the `SAMPLE_RATE` constant (16000) matches what is sent in the MIME type to Gemini.
- The `AudioContext` creates a sample-rate mismatch error if forced to 16kHz on initialization; we accept the system rate and downsample manually.
